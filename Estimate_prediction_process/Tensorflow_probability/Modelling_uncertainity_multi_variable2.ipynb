{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34c789b6-9af6-475b-a22c-a577a262f955",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fed1aa-ffff-4e41-bb4c-474de5d88058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63288a95-147d-44a6-bbc3-5f57ba2d03b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import pickle\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d98834-6399-4bac-b012-14717d8588b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "path = 'C:/Users/nkalimip/Downloads/Estimate_prediction_process/uncertanity_example/information/ABBY_LAI_trainingdata.csv'\n",
    "df = pd.read_csv(path)\n",
    "df = df[[\"B3\",  \"B4\",  \"GVI\", \"NDVI\",  \"NLI\",  \"estimateLAI\"]]\n",
    "print(df)\n",
    "\n",
    "# #normallize\n",
    "# from sklearn import preprocessing\n",
    "# import pandas as pd\n",
    "\n",
    "# # d = preprocessing.normalize(df)\n",
    "# scaler = preprocessing.MinMaxScaler()\n",
    "# d = scaler.fit_transform(df)\n",
    "\n",
    "# df = pd.DataFrame(d, columns=df.columns)\n",
    "# print(df)\n",
    "\n",
    "\n",
    "# pint(n_samples)\n",
    "# column_number = 1\n",
    "X,y = df.loc[:, df.columns != 'estimateLAI'],df.loc[:, df.columns == 'estimateLAI']\n",
    "# X,y = df[\"B4\"], df[\"estimateLAI\"]\n",
    "\n",
    "# X = np.array([[n] for n in X])\n",
    "# y = np.array([[n] for n in y])\n",
    "\n",
    "x_train, x_tst, y_train, y_tst = train_test_split(np.array(X), np.array(y), test_size=0.33)\n",
    "n_samples = len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c82aec08-a38b-48b1-bf7b-10a4e05d490a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73092caa-e2d4-4538-8261-5794b03bce64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neonsl2p:\n",
    "    \n",
    "# document where nane[:16] comes from it is an iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cac4048-4287-4491-9781-57a2792ffa66",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Aleatoric Uncertainity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320b8cb4-eb42-4ade-96c6-bd2ac6a35d43",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Defining Log-Likelihood Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74e139d5-3272-422a-9ab2-ebeb8ad3d72c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "negloglik = lambda y_true, y_pred: -y_pred.log_prob(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b527ad65-64e2-41a2-9f35-91921f834ec3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53b7375c-2528-4e6c-bdfc-e5eb90c6cd25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in biomes:\n",
    "    normalizer = layers.Normalization(input_shape=[5,], axis=None)\n",
    "    normalizer.adapt(np.array(calbiomeDictLAI[i]['X_train']))\n",
    "    \n",
    "    valbiomeDictLAI[i]['nn'] = tf.keras.Sequential([\n",
    "        normalizer,\n",
    "        layers.Dense(80, activation='relu'),\n",
    "        layers.Dense(25, activation='relu'),\n",
    "        layers.Dense(tfpl.IndependentNormal.params_size(1)),\n",
    "        tfpl.IndependentNormal(event_shape=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccb076ec-b9f0-44cf-8eff-d4276dd8c70e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in biomes:\n",
    "    normalizer = layers.Normalization(input_shape=[5,], axis=None)\n",
    "    normalizer.adapt(np.array(calbiomeDictLAI[i]['X_train']))\n",
    "    \n",
    "    valbiomeDictLAI[i]['nn'] = tf.keras.Sequential([\n",
    "        normalizer,\n",
    "        layers.Dense(tfpl.IndependentNormal.params_size(80)),\n",
    "        tfpl.IndependentNormal(event_shape=80),\n",
    "        layers.Dense(tfpl.IndependentNormal.params_size(25)),\n",
    "        tfpl.IndependentNormal(event_shape=25),\n",
    "        layers.Dense(tfpl.IndependentNormal.params_size(1)),\n",
    "        tfpl.IndependentNormal(event_shape=1)])\n",
    "    \n",
    "     valbiomeDictLAI[i]['nn'].compile(optimizer=tf.optimizers.Adam(learning_rate=0.01),\n",
    "                  loss=negloglik,\n",
    "                  metrics=[tf.keras.metrics.RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ebd4ff-5460-490a-a598-263d69f9c4de",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c6f1ac-312c-4141-8ca9-1f9272625797",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in [1]:\n",
    "    callbacks = myCallback()\n",
    "    valbiomeDictLAI[i]['nn_history'] = valbiomeDictLAI[i]['nn'].fit(calbiomeDictLAI[i]['X_train'].to_numpy(), calbiomeDictLAI[i]['y_train'].to_numpy(), epochs=50, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ee30ec-1873-4378-90d7-e7bb4ae136ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "valbiomeDictLAI[i]['DF']['y_nn'] = valbiomeDictLAI[i]['nn'].predict(valbiomeDictLAI[i]['X_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aaaf4e-371a-4a71-a3af-45f9622e5301",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_mean = valbiomeDictLAI[i]['nn'](valbiomeDictLAI[i]['X_test'].to_numpy()).mean()\n",
    "y_sd = valbiomeDictLAI[i]['nn'](valbiomeDictLAI[i]['X_test'].to_numpy()).stddev()\n",
    "\n",
    "y_hat_lower = y_mean - 2 * y_sd\n",
    "y_hat_upper = y_mean + 2 * y_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ea90a1-69d7-451f-a153-33aaec805ebb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(y_sd)\n",
    "print(y_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fc5756-dbb7-4e79-8533-ff660d6427b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "axes = []\n",
    "for i in [1]:\n",
    "    axes.append(plt.scatter(valbiomeDictLAI[i]['y_test'], y_sd, s=8, alpha=0.5, edgecolors='none', label=str(i)))\n",
    "plt.plot([0, 70], [0, 70], 'k--')\n",
    "plt.xlabel(\"Reference LAI\")\n",
    "plt.ylabel(\"Predicted LAI\")\n",
    "plt.title('Neural Network')\n",
    "plt.xlim([0, 70])\n",
    "# plt.ylim([-50, 120])\n",
    "plt.legend(handles=axes, loc='center left', bbox_to_anchor=(1, 0.5), title='Biome')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c40456-6dc0-4495-b909-d5505ef22bb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sklearn.metrics.mean_squared_error(valbiomeDictLAI[1]['y_test'], valbiomeDictLAI[1]['DF']['y_nn'], squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9863f675-a86c-41d2-bdb6-ef688472d732",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "valbiomeDictLAI[2]['nn'].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f090b22a-1887-4664-abfa-12148edb1f0a",
   "metadata": {},
   "source": [
    "# Epistemic Uncertainity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4935dc-4419-4d56-a969-907b96747e62",
   "metadata": {},
   "source": [
    "## Prior Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251b8975-0c57-4d0c-89cb-6a814532e032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prior(kernel_size, bias_size, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    prior_model = tf.keras.Sequential([\n",
    "        tfpl.DistributionLambda(lambda t: tfd.MultivariateNormalDiag(\n",
    "        loc=tf.zeros(n), scale_diag=tf.ones(n)))\n",
    "    ])\n",
    "    return prior_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea57804d-7b8a-4e98-9145-413ad82d38e3",
   "metadata": {},
   "source": [
    "## Posterior Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afb57e8-71e5-40c6-8407-e88ac09e7e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posterior(kernel_size, bias_size, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    posterior_model = tf.keras.Sequential([\n",
    "        tfpl.VariableLayer(tfpl.MultivariateNormalTriL.params_size(n), dtype=dtype),\n",
    "        tfpl.MultivariateNormalTriL(n)\n",
    "    ])\n",
    "    return posterior_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41fe98b-c733-4091-a17d-ef33f1c722b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afefc76-b28a-4cb7-afd9-55f4ba80b291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model, negative-log likelihood as the loss function\n",
    "# and compile the model with the Adam optimizer\n",
    "normalizer = layers.Normalization(input_shape=[5,], axis=None)\n",
    "normalizer.adapt(np.array(calbiomeDictLAI[7]['X_train']))\n",
    "\n",
    "name = 'pnn_8_1'\n",
    "biome = 7\n",
    "\n",
    "valbiomeDictLAI[biome][name] = dict()\n",
    "\n",
    "valbiomeDictLAI[biome][name]['model'] = tf.keras.Sequential([\n",
    "    normalizer,\n",
    "    tfpl.DenseVariational(units=8,\n",
    "                          make_prior_fn=get_prior,\n",
    "                          make_posterior_fn=get_posterior,\n",
    "                          kl_weight=1/calbiomeDictLAI[1]['X_train'].shape[0],\n",
    "                          kl_use_exact=False,\n",
    "                          activation='relu'),\n",
    "    # tfpl.DenseVariational(units=25,\n",
    "    #                       make_prior_fn=get_prior,\n",
    "    #                       make_posterior_fn=get_posterior,\n",
    "    #                       kl_weight=1/calbiomeDictLAI[1]['X_train'].shape[0],\n",
    "    #                       kl_use_exact=False,\n",
    "    #                       activation='relu'),\n",
    "    tfpl.DenseVariational(units=1,\n",
    "                          make_prior_fn=get_prior,\n",
    "                          make_posterior_fn=get_posterior,\n",
    "                          kl_weight=1/calbiomeDictLAI[1]['X_train'].shape[0],\n",
    "                          kl_use_exact=False)\n",
    "])\n",
    "\n",
    "def nll(y_true, y_pred):\n",
    "                   dist = tfp.distributions.Normal(loc=y_pred, scale=1.0)\n",
    "                   return tf.reduce_sum(-dist.log_prob(y_true))\n",
    "\n",
    "valbiomeDictLAI[biome][name]['model'].compile(loss=nll, optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "valbiomeDictLAI[biome][name]['model'].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5d0974-8954-49cf-b379-24d25ca337be",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2c5129-6110-43dc-987a-d512fe4caef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for 50 epochs\n",
    "valbiomeDictLAI[biome][name]['history'] = valbiomeDictLAI[biome][name]['model'].fit(calbiomeDictLAI[biome]['X_train'].to_numpy(), calbiomeDictLAI[biome]['y_train'].to_numpy(), epochs=50, verbose=1)\n",
    "plt.plot(valbiomeDictLAI[biome][name]['history'].history['loss'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced284d7-c814-463d-ab48-27695fd4784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    y_model = valbiomeDictLAI[biome][name]['model'](valbiomeDictLAI[biome]['X_test'])\n",
    "    plt.scatter(valbiomeDictLAI[biome]['y_test'], y_model, alpha=0.5, s=12, lw=0)\n",
    "plt.plot([0, 70], [0, 70], 'k--')\n",
    "plt.xlabel(\"Reference\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
